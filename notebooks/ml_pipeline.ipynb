{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fc30c2b",
   "metadata": {},
   "source": [
    "### Run Processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cf79b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.sklearn import SKLearnModel\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66adc38e",
   "metadata": {},
   "source": [
    "#### Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1706d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "script_path = os.getenv(\"PREPROCESSING_SCRIPT_PATH\")\n",
    "role= os.getenv(\"ROLE\")\n",
    "preprocessing_source_path=os.getenv(\"PREPROCESSING_SOURCE_PATH\")\n",
    "preprocessing_output_path=os.getenv(\"PREPROCESSING_OUTPUT_PATH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef9845f",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2636b101",
   "metadata": {},
   "source": [
    "#### Develop preprocessing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51623636",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "input_columns = [\n",
    "    \"species\",\n",
    "    \"island\",\n",
    "    \"bill_length_mm\",\n",
    "    \"bill_depth_mm\",\n",
    "    \"flipper_length_mm\",\n",
    "    \"body_mass_g\",\n",
    "    \"sex\",\n",
    "]\n",
    "\n",
    "target = \"sex\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parse Arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train-test-split-ratio\", type=float, default=0.3)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    split_ratio = args.train_test_split_ratio\n",
    "    print(\"Received arguments {}\".format(args))\n",
    "\n",
    "    # Process input data\n",
    "    input_data_path = os.path.join(\"/opt/ml/processing/input\", \"penguins.csv\")\n",
    "    print(\"Reading input data from {}\".format(input_data_path))\n",
    "    df = pd.read_csv(input_data_path)\n",
    "    df = pd.DataFrame(data=df, columns=input_columns)\n",
    "    df.dropna(inplace=True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    preprocess = make_column_transformer(\n",
    "        ([\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\"], StandardScaler()),\n",
    "        ([\"species\", \"island\"], OneHotEncoder(sparse=False)),\n",
    "    )\n",
    "\n",
    "    X = preprocess.fit_transform(df.drop(columns=\"sex\"))\n",
    "\n",
    "    # Split data into training and test set\n",
    "    print(\"Splitting data into train and test sets with ratio {}\".format(split_ratio))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        pd.DataFrame(X),\n",
    "        df[target],\n",
    "        test_size=split_ratio,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    train_features_output_path: str = os.path.join(\n",
    "        \"/opt/ml/processing/train\", \"train_features.csv\"\n",
    "    )\n",
    "    train_labels_output_path: str = os.path.join(\n",
    "        \"/opt/ml/processing/train\", \"train_labels.csv\"\n",
    "    )\n",
    "    test_features_output_path: str = os.path.join(\n",
    "        \"/opt/ml/processing/test\", \"test_features.csv\"\n",
    "    )\n",
    "    test_labels_output_path: str = os.path.join(\n",
    "        \"/opt/ml/processing/test\", \"test_labels.csv\"\n",
    "    )\n",
    "\n",
    "    # Save processed data as csv\n",
    "    print(\"Saving training features to {}\".format(train_features_output_path))\n",
    "    X_train.to_csv(train_features_output_path, header=False, index=False)\n",
    "\n",
    "    print(\"Saving test features to {}\".format(test_features_output_path))\n",
    "    X_test.to_csv(test_features_output_path, header=False, index=False)\n",
    "\n",
    "    print(\"Saving training labels to {}\".format(train_labels_output_path))\n",
    "    y_train.to_csv(train_labels_output_path, header=False, index=False)\n",
    "\n",
    "    print(\"Saving test labels to {}\".format(test_labels_output_path))\n",
    "    y_test.to_csv(test_labels_output_path, header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb952960",
   "metadata": {},
   "source": [
    "#### Define & Run SKLearn Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c78291",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.20.0\",\n",
    "    base_job_name=\"preprocessing\",\n",
    "    role=role,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    ")\n",
    "\n",
    "sklearn_processor.run(\n",
    "    code=script_path,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=\"preprocessing.py\", \n",
    "            destination=\"/opt/ml/processing/input\"\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            destination=preprocessing_output_path,\n",
    "            output_name=\"train_data\", \n",
    "            source=\"/opt/ml/processing/train\"\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            destination=preprocessing_output_path,\n",
    "            output_name=\"test_data\", \n",
    "            source=\"/opt/ml/processing/test\"\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "preprocessing_job_description = sklearn_processor.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e942e5fe",
   "metadata": {},
   "source": [
    "#### Inspect generated training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9325ee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features = pd.read_csv(preprocessing_output_path + \"train_features.csv\", nrows=10, header=None)\n",
    "print(\"Training features shape: {}\".format(training_features.shape))\n",
    "training_features.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f082e0d7",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540a63fc",
   "metadata": {},
   "source": [
    "#### Create SKLearn training job "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea5ab7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_and_deploy.py\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "Define model serving functions. More aboutthese functions at:\n",
    "https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/using_sklearn.html#load-a-model\n",
    "\"\"\"\n",
    "def model_fn(model_dir):\n",
    "    model = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return model\n",
    "\n",
    "def input_fn(request_body, content_type):\n",
    "    if content_type == 'text/csv':\n",
    "        samples = []\n",
    "        for r in request_body.split('|'):\n",
    "            samples.append(list(map(float,r.split(','))))\n",
    "        return np.array(samples)\n",
    "    else:\n",
    "        raise ValueError(\"Thie model only supports text/csv input\")\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    return model.predict(input_data)\n",
    "\n",
    "def output_fn(prediction, content_type):\n",
    "    return str(prediction)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    training_data_directory = \"/opt/ml/input/data/train\"\n",
    "    train_features_data = os.path.join(training_data_directory, \"train_features.csv\")\n",
    "    train_labels_data = os.path.join(training_data_directory, \"train_labels.csv\")\n",
    "    print(\"Reading input data\")\n",
    "    X_train = pd.read_csv(train_features_data, header=None)\n",
    "    y_train = pd.read_csv(train_labels_data, header=None)\n",
    "\n",
    "    model = LogisticRegression(class_weight=\"balanced\", solver=\"lbfgs\")\n",
    "    print(\"Training LR model\")\n",
    "    model.fit(X_train, y_train)\n",
    "    model_output_directory = os.path.join(\"/opt/ml/model\", \"model.joblib\")\n",
    "    print(\"Saving model to {}\".format(model_output_directory))\n",
    "    joblib.dump(model, model_output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f5607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn = SKLearn(\n",
    "    entry_point=\"train_and_deploy.py\",\n",
    "    framework_version=\"0.20.0\", \n",
    "    instance_type=\"ml.m5.xlarge\", \n",
    "    role=role\n",
    ")\n",
    "sklearn.fit({\"train\": preprocessing_output_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b7c45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_s3_uri = sklearn.output_path + sklearn.latest_training_job.name + \"/output/model.tar.gz\"\n",
    "model_data_s3_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7ff0cd",
   "metadata": {},
   "source": [
    "#### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1bc0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile evaluate.py\n",
    "\n",
    "import json\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = os.path.join(\"/opt/ml/processing/model\", \"model.tar.gz\")\n",
    "    print(\"Extracting model from path: {}\".format(model_path))\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path=\".\")\n",
    "    print(\"Loading model\")\n",
    "    model = joblib.load(\"model.joblib\")\n",
    "\n",
    "    print(\"Loading test input data\")\n",
    "    test_features_data = os.path.join(\"/opt/ml/processing/test\", \"test_features.csv\")\n",
    "    test_labels_data = os.path.join(\"/opt/ml/processing/test\", \"test_labels.csv\")\n",
    "\n",
    "    X_test = pd.read_csv(test_features_data, header=None)\n",
    "    y_test = pd.read_csv(test_labels_data, header=None)\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    print(\"Creating classification evaluation report\")\n",
    "    report_dict = classification_report(y_test, predictions, output_dict=True)\n",
    "    report_dict[\"accuracy\"] = accuracy_score(y_test, predictions)\n",
    "    # report_dict[\"roc_auc\"] = roc_auc_score(y_test, predictions)\n",
    "\n",
    "    print(\"Classification report:\\n{}\".format(report_dict))\n",
    "\n",
    "    evaluation_output_path = os.path.join(\n",
    "        \"/opt/ml/processing/evaluation\", \"evaluation.json\"\n",
    "    )\n",
    "    print(\"Saving classification report to {}\".format(evaluation_output_path))\n",
    "\n",
    "    with open(evaluation_output_path, \"w\") as f:\n",
    "        f.write(json.dumps(report_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff658d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_processor.run(\n",
    "    code=\"evaluate.py\",\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=model_data_s3_uri, \n",
    "            destination=\"/opt/ml/processing/model\"\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=\"s3://mlops-test-processed-data/\", \n",
    "            destination=\"/opt/ml/processing/test\"),\n",
    "    ],\n",
    "    outputs=[ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\")],\n",
    ")\n",
    "evaluation_job_description = sklearn_processor.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c363c1c6",
   "metadata": {},
   "source": [
    "#### Inspect Evaluation result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0684c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('s3')\n",
    "s3_path=evaluation_job_description[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "bucket, key = s3_path.split(\"//\")[1].split(\"/\",1)\n",
    "result = client.get_object(Bucket=bucket, Key= key + '/evaluation.json') \n",
    "json.loads(result['Body'].read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ab7340",
   "metadata": {},
   "source": [
    "### Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48d1190",
   "metadata": {},
   "source": [
    "#### Deploy Estimator to Sagemaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374e6d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = sklearn.deploy(instance_type='ml.m4.xlarge', initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a65116",
   "metadata": {},
   "source": [
    "#### Test Sagemaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114fa67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the deploy_test data\n",
    "deploy_test = training_features.head(2).values.tolist()\n",
    "\n",
    "# Format the deploy_test data features\n",
    "request_body = \"\"\n",
    "for sample in deploy_test:\n",
    "    request_body += \",\".join([str(n) for n in sample]) + \"|\"\n",
    "request_body = request_body[:-1] \n",
    "print(\"*\"*20)\n",
    "print(f\"Calling Sagemaker Endopint with the following request_body: {request_body}\")\n",
    "\n",
    "# create sagemaker client using boto3\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "# Specify endpoint and content_type\n",
    "endpoint_name = predictor.endpoint\n",
    "content_type = 'text/csv'\n",
    "\n",
    "# Make call to endpoint\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=request_body,\n",
    "    ContentType=content_type\n",
    "    )\n",
    "response_from_endpoint = response['Body'].read().decode(\"utf-8\")\n",
    "print(\"*\"*20)\n",
    "print(f\"Response from Endpoint: {response_from_endpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdcbefc",
   "metadata": {},
   "source": [
    "#### Delete Endpoint, if no longer in use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa581d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0b4c72",
   "metadata": {},
   "source": [
    "## Build REST API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b958ac61",
   "metadata": {},
   "source": [
    "#### Create Lambda Function for handling API <-> Sagemaker Endpoint traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f2cab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile serving_lambda.py\n",
    "import os\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "endpoint_name = os.environ['ENDPOINT_NAME']\n",
    "runtime= boto3.client('runtime.sagemaker')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    print(\"Received event: \" + json.dumps(event, indent=2))\n",
    "    \n",
    "    data = json.loads(json.dumps(event))\n",
    "    payload = json.loads(data['data'])\n",
    "    print(payload)\n",
    "    \n",
    "    # Format the deploy_test data features\n",
    "    request_body = \"\"\n",
    "    for sample in payload:\n",
    "        request_body += \",\".join([str(n) for n in sample]) + \"|\"\n",
    "    request_body = request_body[:-1] \n",
    "    print(\"request_body: \", request_body)\n",
    "    \n",
    "    response = runtime.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                       ContentType='text/csv',\n",
    "                                       Body=request_body)\n",
    "                                       \n",
    "    label = response['Body'].read().decode('utf-8').strip(\"[]\").strip(\"'\")\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2644792",
   "metadata": {},
   "source": [
    "#### Go to API Gateway & Select Create new REST Endpoint\n",
    "\n",
    "![REST API](img/REST.png)\n",
    "\n",
    "#### Choose a name and create a new API\n",
    "\n",
    "![REST API](img/CREATE_NEW.png)\n",
    "\n",
    "#### Create a new method of type POST and choose your lambda as target\n",
    "\n",
    "![REST API](img/POST.png)\n",
    "\n",
    "#### Deploy API\n",
    "\n",
    "![REST API](img/DEPLOY.png)\n",
    "\n",
    "### Go to APIs --> Stages --> Inspect your newly created stage and collect Invocation Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374ab554",
   "metadata": {},
   "source": [
    "#### Invoke Request against REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db532e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = os.getenv(\"API_URL\")\n",
    "payload = json.dumps({\"data\":\"[[-0.6396528091784842, 0.3738717119645826, -0.9980179785096928, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]]\"})\n",
    "print(f\"Calling ML Api with the following payload {payload}\")\n",
    "response = requests.post(url, data=payload)\n",
    "print(\"*\"*20)\n",
    "print(f\"Return Message. Status code: {response.status_code}, Message: {response.text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
