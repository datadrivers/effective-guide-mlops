{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fc30c2b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Building an end-to-end ML Pipeline with AWS Sagemaker & API Gateway\n",
    "\n",
    "This Notebook shows a basic example how to build an end-to-end machine learing pipeline on AWS by using the [AWS Sagemaker Python SDK](https://sagemaker.readthedocs.io/en/stable/). \n",
    "\n",
    "The AWS Sagemaker Components provide a service for running your python scripts in docker containers, using either AWS maintained images or custom images. Additionally the Sagemaker Python SDK implements many convenience functions for handling parameters such as instance size, IO-Handling, and deployment. They are great building blocks for scalable, consistent, reproducable ml pipelines. They can easily be orchestarated by either using an open source Workflow Tool (Airlflow, Prefect) or AWS Step Functions. They provide a perfect fit for ordinary ml workflows with medium sized datasets, tabular data. They support images for the most widely used ml frameworks, eg. scikit-learn, tensorflow and pytorch.\n",
    "\n",
    "The Notebook contains both the source code for preprocessing, training and deployment, as well as the calls to the sagemaker API that are executing the jobs. \n",
    "\n",
    "### The Pipeline\n",
    "\n",
    "##### Data Processing:\n",
    "- The flow begins with a preprocessing script that uses `pandas` and `scikit-learn` to read a csv, apply transformations to the data, splits the data into train and test set, and saves the data to S3.\n",
    "- The preprocessing file will be executed with the `SKLearnProcessor`, where instance size and IO paths will be configured. \n",
    "\n",
    "##### Model Training & Deployment:\n",
    "- Next, another script for model training and deployment will be created. This script includes the algorithm, the training routine, the serialization of the model, and the serving functions that will be used for model deployment.\n",
    "- This script will be executed with the `SKLearn` estimator class. When calling `fit()` on it, model training will be executed. When calling `deploy()`, the model will be deployed. \n",
    "\n",
    "##### Model Serving:\n",
    "- A lmabda function is created as an intermediate layer between your sagemaker model endpoint and your REST API.\n",
    "- A REST API is configured with API Gateway. It consists of a simple `POST` method that calls the lambda function with live data as payload. The live data is passed to the endpoint and predictions are received and returned to the caller. \n",
    "\n",
    "<img src=\"img/flowchart_ml_pipeline.png\" alt=\"Flowchart\" width=\"1200\" height=\"675\" style=\"horizontal-align:middle\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62f4807-96dc-48e4-83fd-408906ff3f26",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "To run this demo, you will need access to an AWS account, a user that let's you access the ressource needed, and roles to grand permessions for the services. This demo will not cover how to set up IAM roles and permissions.\n",
    "\n",
    "To run this demo, you will need access to an AWS account, create a user with a policy that grants permissions to all services that will be used in this example. The notbook can be run on any environment, given that authentication is provided, however the recommended, and easiest way is to run this tutorial on an AWS sagemakeer notebook instance. You can find more information about setting that up [here](https://docs.aws.amazon.com/sagemaker/latest/dg/howitworks-create-ws.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66adc38e",
   "metadata": {},
   "source": [
    "#### Load Environment Variables\n",
    "\n",
    "I am using [dotenv](https://github.com/theskumar/python-dotenv) to handle my environment variables. You could either directly define them in the notebook below, replacing the `os.getenv()` call (e.g. `script_path = \"path/to/your/script\"`), or you define them in an `.env` file in your root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19769c8-57d8-4e82-bb93-c522ee1952c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1706d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "sagemaker_role= os.getenv(\"SAGEMAKER_ROLE\") # Sagemaker Role TODO: Define Separate Roles for SageMaker, Lambda and Gateway\n",
    "raw_data_bucket=os.getenv(\"RAW_DATA_PATH\")\n",
    "processed_data_bucket=os.getenv(\"PREPROCESSING_OUTPUT_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91bacea-9c1b-4f3e-868e-a1dace4ec2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment variables\n",
    "\n",
    "#sagemaker_role = \"XXXXXXXXXXXXXXXX\"\n",
    "#lambda_role = \"XXXXXXXXXXXXXXXX\"\n",
    "#raw_data_path=\"XXXXXXXXXXXXXXXX\"\n",
    "#preprocessing_output_path=\"XXXXXXXXXXXXXXXX\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc4ceb2-7407-4b17-ae51-363ba4c61683",
   "metadata": {},
   "source": [
    "## Get Data\n",
    "\n",
    "In this example we will be using the Plamer Penguin Dataset, which provides a good alternative to the frequently used Iris dataset. It contains information about various penguins. You can read more about it [here](https://allisonhorst.github.io/palmerpenguins/articles/intro.html). \n",
    "\n",
    "The objective we will be solving with our machine learning algorithm is to predict the gender of a penguin by using all other columns as features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52b6f7d-b615-44c0-b4b1-2b7ecf8f42f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from palmerpenguins import load_penguins\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e815682f-9e81-466f-92e8-35d554549678",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install palmerpenguins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6d4bbe-5aa9-4404-86e5-db5e65b95e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = load_penguins()\n",
    "penguins.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfd2a1b-2b3f-4eab-b6ea-3addb305516a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write csv to raw data s3 bucket\n",
    "file_path = os.path.join(raw_data_bucket, \"penguins.csv\")\n",
    "penguins.to_csv(file_path)\n",
    "print(f\"Stored raw data in '{file_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef9845f",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234d2af0-a930-4255-8aa3-77614ae836e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sagemaker classes\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.sklearn import SKLearnModel\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2636b101",
   "metadata": {},
   "source": [
    "#### Develop preprocessing script\n",
    "\n",
    "This is an example preprocessing script. It will read the data into a Pandas DataFrame, and apply a  scikit-learn column transformer pipeline, one-hot-encoding categorical variables and scaling interval-scaled variables. The it will split the data into training and test set and write the data to a flat file.\n",
    "\n",
    "When executing the cell, the magic command `%%writefile filename.py` will save the file the code as a python file in your current working directory. This will allow the SageMaker preprocessing job to use the script in a seperate docker container, where the preprocessing will be executed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51623636",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "features = [\n",
    "    \"bill_length_mm\",\n",
    "    \"bill_depth_mm\",\n",
    "    \"flipper_length_mm\",\n",
    "    \"species\",\n",
    "    \"island\",\n",
    "]\n",
    "\n",
    "target = \"sex\"\n",
    "\n",
    "columns =  features + [target]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parse Arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train-test-split\", type=float, default=0.3)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    split = args.train_test_split\n",
    "    print(\"Arguments {}\".format(args))\n",
    "\n",
    "    # Process input data\n",
    "    input_data_path = os.path.join(\"/opt/ml/processing/input\", \"penguins.csv\")\n",
    "    df = pd.read_csv(input_data_path)\n",
    "    df = pd.DataFrame(data=df, columns=columns)\n",
    "    df.dropna(inplace=True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Create sklearn preprocessing pipeline\n",
    "    preprocess_pipeline = make_column_transformer(\n",
    "        ([\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\"], StandardScaler()),\n",
    "        ([\"species\", \"island\"], OneHotEncoder(sparse=False)),\n",
    "    )\n",
    "\n",
    "    # Apply Pipeline\n",
    "    X = preprocess_pipeline.fit_transform(df.drop(columns=target))\n",
    "\n",
    "    # Split data into training and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        pd.DataFrame(X),\n",
    "        df[target],\n",
    "        test_size=split,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    train_features_output_path: str = os.path.join(\n",
    "        \"/opt/ml/processing/train\", \"train_features.csv\"\n",
    "    )\n",
    "    train_labels_output_path: str = os.path.join(\n",
    "        \"/opt/ml/processing/train\", \"train_labels.csv\"\n",
    "    )\n",
    "    test_features_output_path: str = os.path.join(\n",
    "        \"/opt/ml/processing/test\", \"test_features.csv\"\n",
    "    )\n",
    "    test_labels_output_path: str = os.path.join(\n",
    "        \"/opt/ml/processing/test\", \"test_labels.csv\"\n",
    "    )\n",
    "\n",
    "    # Save processed data as csv\n",
    "    print(\"Training features path {}\".format(train_features_output_path))\n",
    "    X_train.to_csv(train_features_output_path, header=False, index=False)\n",
    "\n",
    "    print(\"Test features path {}\".format(test_features_output_path))\n",
    "    X_test.to_csv(test_features_output_path, header=False, index=False)\n",
    "\n",
    "    print(\"Training labels path {}\".format(train_labels_output_path))\n",
    "    y_train.to_csv(train_labels_output_path, header=False, index=False)\n",
    "\n",
    "    print(\"Test labels path {}\".format(test_labels_output_path))\n",
    "    y_test.to_csv(test_labels_output_path, header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb952960",
   "metadata": {},
   "source": [
    "#### Define & Run SKLearn Preprocessor\n",
    "\n",
    "The `SKLearnProcessor` lets you configure the preprocessing job, including the `framework_version`, the `instance_type` and the number of instances. You could also pass a custom docker image to the object that would be used instead of the scikit-learn image maintained by AWS. \n",
    "\n",
    "When calling `run()` the preprocessing job will be executed. The function accepts the path to the preprocessing script that was defined in the cell above as input. Additionally,  data input and output paths will be defined. S3 buckets can be used for retrieval of raw data and storing of proceeesed data. With the objects `ProccesingInput` and `ProcessingOutput` we make sure that the paths in S3 and in our docker container are mapped accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c78291",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.20.0\",\n",
    "    base_job_name=\"preprocessing\",\n",
    "    role=sagemaker_role,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    ")\n",
    "\n",
    "docker_base_path: str = \"/opt/ml/processing/\"\n",
    "\n",
    "sklearn_processor.run(\n",
    "    code=\"preprocessing.py\",\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=raw_data_bucket, \n",
    "            destination=os.path.join(docker_base_path, \"input\")\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            destination=processed_data_bucket,\n",
    "            output_name=\"train_data\", \n",
    "            source=os.path.join(docker_base_path, \"train\")\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            destination=processed_data_bucket,\n",
    "            output_name=\"test_data\", \n",
    "            source=os.path.join(docker_base_path, \"test\")\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "preprocessing_job_description = sklearn_processor.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e942e5fe",
   "metadata": {},
   "source": [
    "#### Inspect generated training data\n",
    "\n",
    "Let's have a look at our processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9325ee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features = pd.read_csv(processed_data_bucket + \"train_features.csv\", nrows=10, header=None)\n",
    "print(\"Training features shape: {}\".format(training_features.shape))\n",
    "training_features.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f082e0d7",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e906c0-61af-4584-8430-096006cfbc6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create SKLearn training and deploy script\n",
    "\n",
    "In order to execetue model training and deployment of the trained model, we need to write another script. \n",
    "\n",
    "The script will comprise of the training routine, which will ingest the processed training data that was generated in the Sagemaker Processing step above. It reads the data, instanciates the model - here a simple `LogisticRegression` and calls `fit` on the model. The model is then serialized and saved in our working directory. The `SKLearn` object will then move the artifacts to the desired output path in S3. If no output path is specified, Sagemaker will create a new bucket to store the artifacts of the training job.\n",
    "\n",
    "The script also contasins several serving functions that Sagemaker requires for model serving via the sagemaker model endpoint service. These functions comprise of `model_fn()` ensuring that the model gets loaded from file, `input_fn()` handling the input in a way that it can be used for calling the `predict()` function on the model, the `predict_fn()` which actually calls `predict` on the model and the `output_fn()`, which will convert the model output to a format that can be send back to the caller. \n",
    "\n",
    "The script will also be saved to disc with the `%%writefile` magic command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a090c081-f350-4536-9ad4-48bb3041f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_and_deploy.py\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "Define model serving functions. More aboutthese functions at:\n",
    "https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/using_sklearn.html#load-a-model\n",
    "\"\"\"\n",
    "def model_fn(model_dir):\n",
    "    model = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return model\n",
    "\n",
    "def input_fn(request_body, content_type):\n",
    "    if content_type == 'text/csv':\n",
    "        samples = []\n",
    "        for r in request_body.split('|'):\n",
    "            samples.append(list(map(float,r.split(','))))\n",
    "        return np.array(samples)\n",
    "    else:\n",
    "        raise ValueError(\"Thie model only supports text/csv input\")\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    return model.predict(input_data)\n",
    "\n",
    "def output_fn(prediction, content_type):\n",
    "    return str(prediction)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    training_data_directory = \"/opt/ml/input/data/train\"\n",
    "    train_features_data = os.path.join(training_data_directory, \"train_features.csv\")\n",
    "    train_labels_data = os.path.join(training_data_directory, \"train_labels.csv\")\n",
    "\n",
    "    X_train = pd.read_csv(train_features_data, header=None)\n",
    "    y_train = pd.read_csv(train_labels_data, header=None)\n",
    "\n",
    "    model = LogisticRegression(class_weight=\"balanced\", solver=\"lbfgs\")\n",
    "    model.fit(X_train, y_train)\n",
    "    model_output_directory = os.path.join(\"/opt/ml/model\", \"model.joblib\")\n",
    "    print(\"Model saing path {}\".format(model_output_directory))\n",
    "    joblib.dump(model, model_output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad4e0cc-a59a-49ed-aad2-90cd6c5195bd",
   "metadata": {},
   "source": [
    "The `SKLearn` object is the standard interface for scheduling and defining model training and deployment of scikit-learn models. After specifying the ressources needed, the framework version and the entry_point, we can call `fit()` in order to execute the training job. We pass a dictionary with a single keyword `\"train\"` that specifies the path to the processed data in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f5607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn = SKLearn(\n",
    "    entry_point=\"train_and_deploy.py\",\n",
    "    framework_version=\"0.20.0\", \n",
    "    instance_type=\"ml.m5.xlarge\", \n",
    "    role=sagemaker_role\n",
    ")\n",
    "sklearn.fit({\"train\": processed_data_bucket})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05d7214-b931-43cf-9ff3-dbdd67463353",
   "metadata": {},
   "source": [
    "#### Evaluate Model Performance\n",
    "Another script is created in order to evaluate the perfomance of the model created above. The evaluation step will again be executed as an individual step in our ml pipeline. It loads both the model and the processed test data, collects several metrics (classification report, roc_auc score, accuracy) and stores them in a JSON file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d737a1-7751-4e94-89b1-1b51f7f7ae77",
   "metadata": {},
   "source": [
    "Because we did not specify a bucket, where our model artifact should be stored, the training job created a new one. The uri can be retrieved from the metadata contained in the `sklearn` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7db7b21-8386-4827-a2db-ec8f7515c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model data in order to load model\n",
    "model_data_s3_uri = sklearn.output_path + sklearn.latest_training_job.name + \"/output/model.tar.gz\"\n",
    "model_data_s3_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a05234-c75d-472e-9ff6-58a7ade4f431",
   "metadata": {},
   "source": [
    "\n",
    "Execute model evaluation using the same processing configurations as for the preprocessing job and the same object instantiated above. Two inputs are specified, one for the model and another one for the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e3e31a-2d74-49e0-b4bc-27c0a603cc89",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Evaluate Model Performance\n",
    "\n",
    "Another script is created in order to evaluate the perfomance of the model created above. The evaluation step will again be executed as an individual step in our ml pipeline. It loads both the model and the processed test data, collects several metrics (classification report, roc_auc score, accuracy) and stores them in a JSON file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1bc0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile evaluate.py\n",
    "\n",
    "import json\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = os.path.join(\"/opt/ml/processing/model\", \"model.tar.gz\")\n",
    "\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path=\".\")\n",
    "\n",
    "    model = joblib.load(\"model.joblib\")\n",
    "\n",
    "    features = os.path.join(\"/opt/ml/processing/test\", \"test_features.csv\")\n",
    "    labels = os.path.join(\"/opt/ml/processing/test\", \"test_labels.csv\")\n",
    "\n",
    "    X_test = pd.read_csv(features, header=None)\n",
    "    y_test = pd.read_csv(labels, header=None)\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    report = classification_report(y_test, predictions, output_dict=True)\n",
    "    report[\"accuracy\"] = accuracy_score(y_test, predictions)\n",
    "\n",
    "    eval_output_path = os.path.join(\n",
    "        \"/opt/ml/processing/evaluation\", \"evaluation.json\"\n",
    "    )\n",
    "    print(\"Evaluation output path: {}\".format(eval_output_path))\n",
    "\n",
    "    with open(eval_output_path, \"w\") as f:\n",
    "        f.write(json.dumps(report))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f8e8f8-ba0e-4233-95ad-0481ccf89f66",
   "metadata": {},
   "source": [
    "Execute model evaluation using the same processing configurations as for the preprocessing job and the same object instantiated above. Two inputs are specified, one for the model and another one for the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab77371b-b6b2-46b5-825d-06b69f410ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "sklearn_processor.run(\n",
    "    code=\"evaluate.py\",\n",
    "    inputs=[\n",
    "        ProcessingInput(source=model_data_s3_uri, destination=\"/opt/ml/processing/model\"),\n",
    "        ProcessingInput(source=processed_data_bucket, destination=\"/opt/ml/processing/test\"),\n",
    "    ],\n",
    "    outputs=[ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\")],\n",
    ")\n",
    "evaluation_job_description = sklearn_processor.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b751e9a-65a3-43ee-8ee9-d9ebb92f7d6d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Inspect Evaluation result\n",
    "\n",
    "The JSON file that was created in the evaluation job can now be read and inspected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0aa041-8d78-44b4-a747-350fc6d25ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "client = boto3.client('s3')\n",
    "s3_path=evaluation_job_description[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "bucket, key = s3_path.split(\"//\")[1].split(\"/\",1)\n",
    "result = client.get_object(Bucket=bucket, Key= key + '/evaluation.json') \n",
    "json.loads(result['Body'].read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ab7340",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c654a63-7a4c-47b0-9a6f-22cb6fff7b1a",
   "metadata": {},
   "source": [
    "#### Deploy Estimator to Sagemaker Endpoint\n",
    "\n",
    "After evaluating our model, we can now go on and deploy it. In order to do so, we only have to call `deploy()` on the `sklearn` object that we used for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa68d1bd-1702-4354-bb50-016c433c4b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = sklearn.deploy(instance_type='ml.m4.xlarge', initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a65116",
   "metadata": {},
   "source": [
    "#### Test Sagemaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ad43bd-d098-4088-8c11-75b803be8de2",
   "metadata": {},
   "source": [
    "We can now run our first test against our model endpoint directly from our jupyter notebook. To do so, we can simply take some of the training features, add them to a request and then call our model by using the Sagemaker client with the `invoce_endpoint` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114fa67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in two rows from the training data\n",
    "training_data = training_features.head(2).values.tolist()\n",
    "\n",
    "# Format the deploy_test data features\n",
    "request_body = \"\"\n",
    "for sample in training_data:\n",
    "    request_body += \",\".join([str(n) for n in sample]) + \"|\"\n",
    "request_body = request_body[:-1] \n",
    "print(\"*\"*20)\n",
    "print(f\"Calling Sagemaker Endopint with the following request_body: {request_body}\")\n",
    "\n",
    "# create sagemaker client using boto3\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "# Specify endpoint and content_type\n",
    "endpoint_name = predictor.endpoint\n",
    "content_type = 'text/csv'\n",
    "\n",
    "# Make call to endpoint\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=request_body,\n",
    "    ContentType=content_type\n",
    "    )\n",
    "response_from_endpoint = response['Body'].read().decode(\"utf-8\")\n",
    "print(\"*\"*20)\n",
    "print(f\"Response from Endpoint: {response_from_endpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdcbefc",
   "metadata": {},
   "source": [
    "#### Delete Endpoint, if no longer in use\n",
    "\n",
    "Because your endpoint has incurring costs while in use, it is advisable to delete it as soon as it is no longer needed. If you follow this tutorial for testing purposes, make sure that your endpoint is deleted as soon as you stop working on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa581d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This call will delete the endpoint\n",
    "# predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f590466f-5487-44de-b642-c40cb3174a25",
   "metadata": {},
   "source": [
    "Beware that directly calling the model endpoint should only be done for testing purposes. If you want to make your model available for live predictions, it is advisable to add a proper REST API that handles incoming requests. How this can be done, will be described in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0b4c72",
   "metadata": {},
   "source": [
    "## Build REST API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b958ac61",
   "metadata": {},
   "source": [
    "#### Create Lambda Function for handling API <-> Sagemaker Endpoint traffic\n",
    "\n",
    "First we will write a lambda function for handling the traffic between our REST API and our model enpoint. It will be receiving requests from the API as input, invoke the model endpoint and return the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f2cab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile serving_lambda.py\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "endpoint_name = os.environ['ENDPOINT_NAME']\n",
    "runtime= boto3.client('runtime.sagemaker')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    print(\"Received event: \" + json.dumps(event, indent=2))\n",
    "    \n",
    "    data = json.loads(json.dumps(event))\n",
    "    payload = json.loads(data['data'])\n",
    "    print(payload)\n",
    "    \n",
    "    # Format the data so that it can be processed by the model endpoint\n",
    "    body = \"\"\n",
    "    for sample in payload:\n",
    "        body += \",\".join([str(n) for n in sample]) + \"|\"\n",
    "    body = body[:-1] \n",
    "    print(\"request_body: \", body)\n",
    "    \n",
    "    response = runtime.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                       ContentType='text/csv',\n",
    "                                       Body=body)\n",
    "                                       \n",
    "    label = response['Body'].read().decode('utf-8').strip(\"[]\").strip(\"'\")\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e425f01f-18b9-4cfa-a973-516f7345257b",
   "metadata": {},
   "source": [
    "Print out predictor endpoint and add string to lambda function as environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e413828-efe2-43cb-bad0-fee5040ec7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2644792",
   "metadata": {},
   "source": [
    "#### Configure API Gateway\n",
    "\n",
    "This tutrial will walk you through setting up API Gateway via the management console. If you whish to run this in production, it is advisable to provision and configure this ressource with an infrastructure management tool, such as AWS Cloud Formation or Terraform. \n",
    "\n",
    "##### Step I: Go to API Gateway & Select Create new REST Endpoint\n",
    "\n",
    "![REST API](img/REST.png)\n",
    "\n",
    "##### Step II: Choose a name and create a new API\n",
    "\n",
    "![REST API](img/CREATE_NEW.png)\n",
    "\n",
    "##### Step III: Create a new method of type POST and choose your lambda as target\n",
    "\n",
    "![REST API](img/POST.png)\n",
    "\n",
    "##### Step IV: Deploy API\n",
    "\n",
    "![REST API](img/DEPLOY.png)\n",
    "\n",
    "##### Step V: Go to APIs --> Stages --> Inspect your newly created stage and collect Invocation Endpoint\n",
    "Set invocation endpoint URL as environemnt variable `\"API_URL\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374ab554",
   "metadata": {},
   "source": [
    "#### Invoke Request against REST API\n",
    "\n",
    "After you have successfully created your REST API with API Gateway, you can now test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db532e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = os.getenv(\"API_URL\")\n",
    "payload = json.dumps({\"data\":\"[[-0.6396528091784842, 0.3738717119645826, -0.9980179785096928, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]]\"})\n",
    "print(f\"Calling model REST API with the following payload {payload}\")\n",
    "response = requests.post(url, data=payload)\n",
    "print(\"*\"*20)\n",
    "print(f\"Return Message. Status code: {response.status_code}, Message: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942ca347-de2c-4ba7-ba68-ef6fdca403f3",
   "metadata": {},
   "source": [
    "### Outro \n",
    "\n",
    "That's it. After following all steps, you should now have successfully created an end-to-end ml pipeline with AWS Sagemaker and configured a REST API that serves your predictions online. WOW!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
