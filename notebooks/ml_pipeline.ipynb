{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fc30c2b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Building an end-to-end ML Pipeline with AWS Sagemaker & API\n",
    "\n",
    "This Notebook shows a basic example how to build an end-to-end machine learing pipeline on AWS by using the [AWS Sagemaker Python SDK](https://sagemaker.readthedocs.io/en/stable/). \n",
    "\n",
    "The AWS Sagemaker Components provide a service for running your python scripts in docker containers, using either AWS maintained images or custom images. Additionally the Sagemaker Python SDK implements many convenience functions for handling parameters such as instance size, IO-Handling, and deployment. They are great building blocks for scalable, consistent, reproducable ml pipelines. They can easily be orchestarated by either using an open source Workflow Tool (Airlflow, Prefect) or AWS Step Functions. They provide a perfect fit for ordinary ml workflows with medium sized datasets, tabular data. They support images for the most widely used ml frameworks, eg. scikit-learn, tensorflow and pytorch.\n",
    "\n",
    "The Notebook contains both the source code for preprocessing, training and deployment, as well as the calls to the sagemaker API that are executing the jobs. \n",
    "\n",
    "### The Pipeline\n",
    "\n",
    "##### Data Processing:\n",
    "- The flow begins with a preprocessing script that uses `pandas` and `scikit-learn` to read a csv, apply transformations to the data, splits the data into train and test set, and saves the data to S3.\n",
    "- The preprocessing file will be executed with the `SKLearnProcessor`, where instance size and IO paths will be configured. \n",
    "\n",
    "##### Model Training & Deployment:\n",
    "- Next, another script for model training and deployment will be created. This script includes the algorithm, the training rutine, the serialization of the model, and the serving functions that will be used for model deployment.\n",
    "- This script will be executed with the `SKLearn` estimator class. When calling `fit()` on it, model training will be executed. When calling `deploy()`, the model will be deployed. \n",
    "\n",
    "##### Model Serving:\n",
    "- A lmabda function is created as an intermediate layer between your sagemaker model endpoint and your REST API.\n",
    "- A REST API is configured with API Gateway. It consists of a simple `POST` method that calls the lambda function with live data as payload. The live data is passed to the enpoint and predictions are receid and returned to the caller. \n",
    "\n",
    "<img src=\"img/flowchart_ml_pipeline.png\" alt=\"Flowchart\" width=\"1200\" height=\"675\" style=\"horizontal-align:middle\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62f4807-96dc-48e4-83fd-408906ff3f26",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "To run this demo, you will need access to an AWS account, a user that let's you access the ressource needed, and roles to grand permessions for the services. \n",
    "\n",
    "This demo will not cover how to set up IAM roles and permissions. It will assume that the reader will be a\n",
    "\n",
    "\n",
    "To run this demo, you will need access to an AWS account, create a user with a policy that grants permissions to all services that will be used in this example. The notbook can be run on any environment, given that authentication is provided, however the recommended, and easiest way is to run this tutorial on an AWS sagemakeer notebook instance. You can find more information about setting that up [here](https://docs.aws.amazon.com/sagemaker/latest/dg/howitworks-create-ws.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c102f4c4-2848-46e0-acfe-758abffbb4e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a8afd2-e88b-4208-97ca-e9ee26287ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write about IAM configuration needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d904d7b-490c-41a4-b8b1-c4986b8ecfed",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cf79b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.sklearn import SKLearnModel\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66adc38e",
   "metadata": {},
   "source": [
    "#### Load Environment Variables\n",
    "\n",
    "I am using [dotenv](https://github.com/theskumar/python-dotenv) to handle my environment variables. You could either directly define them in the notebook below, replacing the `os.getenv()` call (e.g. `script_path = \"path/to/your/script\"`), or you define them in an `.env` file in your root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1706d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "role= os.getenv(\"ROLE\") # Sagemaker Role TODO: Define Separate Roles for SageMaker, Lambda and Gateway\n",
    "preprocessing_source_path=os.getenv(\"PREPROCESSING_SOURCE_PATH\")\n",
    "preprocessing_output_path=os.getenv(\"PREPROCESSING_OUTPUT_PATH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef9845f",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2636b101",
   "metadata": {},
   "source": [
    "#### Develop preprocessing script\n",
    "\n",
    "This is an example preprocessing script. It will read the data into a Pandas DataFrame, and apply a  scikit-learn column transformer pipeline, one-hot-encoding categorical variables and scaling interval-scaled variables. The it will split the data into training and test set and write the data to a flat file.\n",
    "\n",
    "When executing the cell, the magic command `%%writefile filename.py` will save the file the code as a python file in your current working directory. This will allow the SageMaker preprocessing job to use the script in a seperate docker container, where the preprocessing will be executed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51623636",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "input_columns = [\n",
    "    \"species\",\n",
    "    \"island\",\n",
    "    \"bill_length_mm\",\n",
    "    \"bill_depth_mm\",\n",
    "    \"flipper_length_mm\",\n",
    "    \"body_mass_g\",\n",
    "    \"sex\",\n",
    "]\n",
    "\n",
    "target = \"sex\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parse Arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train-test-split-ratio\", type=float, default=0.3)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    split_ratio = args.train_test_split_ratio\n",
    "    print(\"Received arguments {}\".format(args))\n",
    "\n",
    "    # Process input data\n",
    "    input_data_path = os.path.join(\"/opt/ml/processing/input\", \"penguins.csv\")\n",
    "    print(\"Reading input data from {}\".format(input_data_path))\n",
    "    df = pd.read_csv(input_data_path)\n",
    "    df = pd.DataFrame(data=df, columns=input_columns)\n",
    "    df.dropna(inplace=True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    preprocess = make_column_transformer(\n",
    "        ([\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\"], StandardScaler()),\n",
    "        ([\"species\", \"island\"], OneHotEncoder(sparse=False)),\n",
    "    )\n",
    "\n",
    "    X = preprocess.fit_transform(df.drop(columns=\"sex\"))\n",
    "\n",
    "    # Split data into training and test set\n",
    "    print(\"Splitting data into train and test sets with ratio {}\".format(split_ratio))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        pd.DataFrame(X),\n",
    "        df[target],\n",
    "        test_size=split_ratio,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    train_features_output_path: str = os.path.join(\n",
    "        \"/opt/ml/processing/train\", \"train_features.csv\"\n",
    "    )\n",
    "    train_labels_output_path: str = os.path.join(\n",
    "        \"/opt/ml/processing/train\", \"train_labels.csv\"\n",
    "    )\n",
    "    test_features_output_path: str = os.path.join(\n",
    "        \"/opt/ml/processing/test\", \"test_features.csv\"\n",
    "    )\n",
    "    test_labels_output_path: str = os.path.join(\n",
    "        \"/opt/ml/processing/test\", \"test_labels.csv\"\n",
    "    )\n",
    "\n",
    "    # Save processed data as csv\n",
    "    print(\"Saving training features to {}\".format(train_features_output_path))\n",
    "    X_train.to_csv(train_features_output_path, header=False, index=False)\n",
    "\n",
    "    print(\"Saving test features to {}\".format(test_features_output_path))\n",
    "    X_test.to_csv(test_features_output_path, header=False, index=False)\n",
    "\n",
    "    print(\"Saving training labels to {}\".format(train_labels_output_path))\n",
    "    y_train.to_csv(train_labels_output_path, header=False, index=False)\n",
    "\n",
    "    print(\"Saving test labels to {}\".format(test_labels_output_path))\n",
    "    y_test.to_csv(test_labels_output_path, header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb952960",
   "metadata": {},
   "source": [
    "#### Define & Run SKLearn Preprocessor\n",
    "\n",
    "The `SKLearnProcessor` is the object that lets you configure the preprocessing job, including the framework_version, the instance_type and the number of instances. You could also pass a custom docker image to the object that would be used instead of the scikit-learn image maintained by AWS. \n",
    "\n",
    "When calling `run()` the preprocessing job will be executed. The function accepts the path to the preprocessing script that was defined in the cell above as input. Additionally,  data input and output paths will be defined. S3 buckets can be used for retrieval of raw data and storing of proceeesed data. With the objects `ProccesingInput` and `ProcessingOutput` we make sure that the paths in S3 and in our docker container are mapped accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c78291",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.20.0\",\n",
    "    base_job_name=\"preprocessing\",\n",
    "    role=role,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    ")\n",
    "\n",
    "docker_base_path: str = \"/opt/ml/processing/\"\n",
    "\n",
    "sklearn_processor.run(\n",
    "    code=\"preprocessing.py\",\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=preprocessing_source_path, \n",
    "            destination=os.path.join(docker_base_path, \"input\")\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            destination=preprocessing_output_path,\n",
    "            output_name=\"train_data\", \n",
    "            source=os.path.join(docker_base_path, \"train\")\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            destination=preprocessing_output_path,\n",
    "            output_name=\"test_data\", \n",
    "            source=os.path.join(docker_base_path, \"test\")\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "preprocessing_job_description = sklearn_processor.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e942e5fe",
   "metadata": {},
   "source": [
    "#### Inspect generated training data\n",
    "\n",
    "Let's have a look at our processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9325ee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features = pd.read_csv(preprocessing_output_path + \"train_features.csv\", nrows=10, header=None)\n",
    "print(\"Training features shape: {}\".format(training_features.shape))\n",
    "training_features.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f082e0d7",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540a63fc",
   "metadata": {},
   "source": [
    "#### Create SKLearn training and deploy script\n",
    "\n",
    "Next thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea5ab7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_and_deploy.py\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "Define model serving functions. More aboutthese functions at:\n",
    "https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/using_sklearn.html#load-a-model\n",
    "\"\"\"\n",
    "def model_fn(model_dir):\n",
    "    model = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return model\n",
    "\n",
    "def input_fn(request_body, content_type):\n",
    "    if content_type == 'text/csv':\n",
    "        samples = []\n",
    "        for r in request_body.split('|'):\n",
    "            samples.append(list(map(float,r.split(','))))\n",
    "        return np.array(samples)\n",
    "    else:\n",
    "        raise ValueError(\"Thie model only supports text/csv input\")\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    return model.predict(input_data)\n",
    "\n",
    "def output_fn(prediction, content_type):\n",
    "    return str(prediction)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    training_data_directory = \"/opt/ml/input/data/train\"\n",
    "    train_features_data = os.path.join(training_data_directory, \"train_features.csv\")\n",
    "    train_labels_data = os.path.join(training_data_directory, \"train_labels.csv\")\n",
    "    print(\"Reading input data\")\n",
    "    X_train = pd.read_csv(train_features_data, header=None)\n",
    "    y_train = pd.read_csv(train_labels_data, header=None)\n",
    "\n",
    "    model = LogisticRegression(class_weight=\"balanced\", solver=\"lbfgs\")\n",
    "    print(\"Training LR model\")\n",
    "    model.fit(X_train, y_train)\n",
    "    model_output_directory = os.path.join(\"/opt/ml/model\", \"model.joblib\")\n",
    "    print(\"Saving model to {}\".format(model_output_directory))\n",
    "    joblib.dump(model, model_output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f5607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn = SKLearn(\n",
    "    entry_point=\"train_and_deploy.py\",\n",
    "    framework_version=\"0.20.0\", \n",
    "    instance_type=\"ml.m5.xlarge\", \n",
    "    role=role\n",
    ")\n",
    "sklearn.fit({\"train\": preprocessing_output_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b7c45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_s3_uri = sklearn.output_path + sklearn.latest_training_job.name + \"/output/model.tar.gz\"\n",
    "model_data_s3_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7ff0cd",
   "metadata": {},
   "source": [
    "#### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1bc0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile evaluate.py\n",
    "\n",
    "import json\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = os.path.join(\"/opt/ml/processing/model\", \"model.tar.gz\")\n",
    "    print(\"Extracting model from path: {}\".format(model_path))\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path=\".\")\n",
    "    print(\"Loading model\")\n",
    "    model = joblib.load(\"model.joblib\")\n",
    "\n",
    "    print(\"Loading test input data\")\n",
    "    test_features_data = os.path.join(\"/opt/ml/processing/test\", \"test_features.csv\")\n",
    "    test_labels_data = os.path.join(\"/opt/ml/processing/test\", \"test_labels.csv\")\n",
    "\n",
    "    X_test = pd.read_csv(test_features_data, header=None)\n",
    "    y_test = pd.read_csv(test_labels_data, header=None)\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    print(\"Creating classification evaluation report\")\n",
    "    report_dict = classification_report(y_test, predictions, output_dict=True)\n",
    "    report_dict[\"accuracy\"] = accuracy_score(y_test, predictions)\n",
    "    # report_dict[\"roc_auc\"] = roc_auc_score(y_test, predictions)\n",
    "\n",
    "    print(\"Classification report:\\n{}\".format(report_dict))\n",
    "\n",
    "    evaluation_output_path = os.path.join(\n",
    "        \"/opt/ml/processing/evaluation\", \"evaluation.json\"\n",
    "    )\n",
    "    print(\"Saving classification report to {}\".format(evaluation_output_path))\n",
    "\n",
    "    with open(evaluation_output_path, \"w\") as f:\n",
    "        f.write(json.dumps(report_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff658d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_processor.run(\n",
    "    code=\"evaluate.py\",\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=model_data_s3_uri, \n",
    "            destination=\"/opt/ml/processing/model\"\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=\"s3://mlops-test-processed-data/\", \n",
    "            destination=\"/opt/ml/processing/test\"),\n",
    "    ],\n",
    "    outputs=[ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\")],\n",
    ")\n",
    "evaluation_job_description = sklearn_processor.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c363c1c6",
   "metadata": {},
   "source": [
    "#### Inspect Evaluation result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0684c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('s3')\n",
    "s3_path=evaluation_job_description[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "bucket, key = s3_path.split(\"//\")[1].split(\"/\",1)\n",
    "result = client.get_object(Bucket=bucket, Key= key + '/evaluation.json') \n",
    "json.loads(result['Body'].read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ab7340",
   "metadata": {},
   "source": [
    "### Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48d1190",
   "metadata": {},
   "source": [
    "#### Deploy Estimator to Sagemaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374e6d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = sklearn.deploy(instance_type='ml.m4.xlarge', initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a65116",
   "metadata": {},
   "source": [
    "#### Test Sagemaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114fa67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the deploy_test data\n",
    "deploy_test = training_features.head(2).values.tolist()\n",
    "\n",
    "# Format the deploy_test data features\n",
    "request_body = \"\"\n",
    "for sample in deploy_test:\n",
    "    request_body += \",\".join([str(n) for n in sample]) + \"|\"\n",
    "request_body = request_body[:-1] \n",
    "print(\"*\"*20)\n",
    "print(f\"Calling Sagemaker Endopint with the following request_body: {request_body}\")\n",
    "\n",
    "# create sagemaker client using boto3\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "# Specify endpoint and content_type\n",
    "endpoint_name = predictor.endpoint\n",
    "content_type = 'text/csv'\n",
    "\n",
    "# Make call to endpoint\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=request_body,\n",
    "    ContentType=content_type\n",
    "    )\n",
    "response_from_endpoint = response['Body'].read().decode(\"utf-8\")\n",
    "print(\"*\"*20)\n",
    "print(f\"Response from Endpoint: {response_from_endpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdcbefc",
   "metadata": {},
   "source": [
    "#### Delete Endpoint, if no longer in use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa581d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0b4c72",
   "metadata": {},
   "source": [
    "## Build REST API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b958ac61",
   "metadata": {},
   "source": [
    "#### Create Lambda Function for handling API <-> Sagemaker Endpoint traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f2cab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile serving_lambda.py\n",
    "import os\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "endpoint_name = os.environ['ENDPOINT_NAME']\n",
    "runtime= boto3.client('runtime.sagemaker')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    print(\"Received event: \" + json.dumps(event, indent=2))\n",
    "    \n",
    "    data = json.loads(json.dumps(event))\n",
    "    payload = json.loads(data['data'])\n",
    "    print(payload)\n",
    "    \n",
    "    # Format the deploy_test data features\n",
    "    request_body = \"\"\n",
    "    for sample in payload:\n",
    "        request_body += \",\".join([str(n) for n in sample]) + \"|\"\n",
    "    request_body = request_body[:-1] \n",
    "    print(\"request_body: \", request_body)\n",
    "    \n",
    "    response = runtime.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                       ContentType='text/csv',\n",
    "                                       Body=request_body)\n",
    "                                       \n",
    "    label = response['Body'].read().decode('utf-8').strip(\"[]\").strip(\"'\")\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2644792",
   "metadata": {},
   "source": [
    "#### Go to API Gateway & Select Create new REST Endpoint\n",
    "\n",
    "![REST API](img/REST.png)\n",
    "\n",
    "#### Choose a name and create a new API\n",
    "\n",
    "![REST API](img/CREATE_NEW.png)\n",
    "\n",
    "#### Create a new method of type POST and choose your lambda as target\n",
    "\n",
    "![REST API](img/POST.png)\n",
    "\n",
    "#### Deploy API\n",
    "\n",
    "![REST API](img/DEPLOY.png)\n",
    "\n",
    "### Go to APIs --> Stages --> Inspect your newly created stage and collect Invocation Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374ab554",
   "metadata": {},
   "source": [
    "#### Invoke Request against REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db532e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = os.getenv(\"API_URL\")\n",
    "payload = json.dumps({\"data\":\"[[-0.6396528091784842, 0.3738717119645826, -0.9980179785096928, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]]\"})\n",
    "print(f\"Calling ML Api with the following payload {payload}\")\n",
    "response = requests.post(url, data=payload)\n",
    "print(\"*\"*20)\n",
    "print(f\"Return Message. Status code: {response.status_code}, Message: {response.text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
