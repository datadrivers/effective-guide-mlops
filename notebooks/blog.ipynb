{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81cdb473",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "see WORD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed534af",
   "metadata": {},
   "source": [
    "# Prerequisites \n",
    "\n",
    "Another advantage of using managed cloud services is that you will need only little configurations to get up and running. Compared to managing compute resources manually, this allows you to focus more on the ml task itself. **(There is a typical word for this cloud benefit --> Look it up)**\n",
    "\n",
    "To implement such a solution yourself, you will need access to an AWS account, create a user with a policy that grants permissions to all services that will be used in this example. The code can be run on any environment, given that authentication is provided, however the easiest way would be to use an AWS Sagemaker notebook instance. You can find more information about setting that up [here](https://docs.aws.amazon.com/sagemaker/latest/dg/howitworks-create-ws.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccac7297",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "In this example we will be using the Plamer Penguin Dataset, which provides a suitable alternative to the frequently used Iris dataset. It contains information about various penguins. You can read more about it [here](https://allisonhorst.github.io/palmerpenguins/articles/intro.html). The objective we will be solving with our machine learning algorithm is to predict the gender of a penguin by using all other columns as features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4f9a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from palmerpenguins import load_penguins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c287bc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = load_penguins()\n",
    "penguins.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dad42f1",
   "metadata": {},
   "source": [
    "Minimal data procesing is required: We drop entries with null values as well as duplicates. Then, we perform a train-test-split and save the data on our working directory as well as on s3 storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734d87cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d891cad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_data_storage_path = \"s3://sandbox-carsten-123/data/\"\n",
    "s3_output_storage_path = \"s3://sandbox-carsten-123/model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec18feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins.dropna(inplace=True)\n",
    "penguins.drop_duplicates(inplace=True)\n",
    "\n",
    "features = [\n",
    "    \"bill_length_mm\",\n",
    "    \"bill_depth_mm\",\n",
    "    \"flipper_length_mm\",\n",
    "    \"species\",\n",
    "    \"island\",\n",
    "]\n",
    "\n",
    "target = \"sex\"\n",
    "\n",
    "test_amount = 0.3\n",
    "train = [np.random.uniform() >= test_amount for _ in range(len(penguins))]\n",
    "test = [not train_flag for train_flag in train]\n",
    "\n",
    "X_train = penguins[train][features]\n",
    "y_train = penguins[train][target]\n",
    "X_test = penguins[test][features]\n",
    "y_test = penguins[test][target]\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"data/\")\n",
    "    print(\"Created data/ directory.\")\n",
    "except:\n",
    "    print(\"Data directory already exists.\")\n",
    "\n",
    "for raw_data_bucket in [\"data/\", s3_data_storage_path]:\n",
    "\n",
    "    X_train.to_csv(os.path.join(raw_data_bucket, \"X_train.csv\"), index=False)\n",
    "    y_train.to_csv(os.path.join(raw_data_bucket, \"y_train.csv\"), index=False)\n",
    "    X_test.to_csv(os.path.join(raw_data_bucket, \"X_test.csv\"), index=False)\n",
    "    y_test.to_csv(os.path.join(raw_data_bucket, \"y_test.csv\"), index=False)\n",
    "    print(f\"Stored data in '{raw_data_bucket}' .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279c41cc",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "\n",
    "To execute model training and deployment of the trained model, we need to write a script comprising of the training routine. \n",
    "The crucial part for the training lies in the *__main__* clause. \n",
    "It reads the data, instanciates a pipeline and trains the the model. Here, a minimal preprocessing of one-hot-encoding and standard scaling is chosen. LogisticRegression acts as a baseline model. The model is then serialized and saved given the model directory. \n",
    "\n",
    "The script takes four arguments. First, we need to define input path for the training data. It assumes the existence of two files: X_train.csv and y_train.csv. The differentiation between categorical and numerical variables is explicitly given in these example. Finally, the output path for the serialized model is defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c372a236",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_and_deploy.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "import argparse\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    model = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return model\n",
    "\n",
    "def float_if_number(entry):\n",
    "    try:\n",
    "        return float(entry)\n",
    "    except:\n",
    "        return entry\n",
    "\n",
    "def input_fn(request_body, content_type):\n",
    "    if content_type == 'text/csv':\n",
    "        samples = []\n",
    "        for r in request_body.split('|'):\n",
    "            print(r)\n",
    "            samples.append(list(map(float_if_number,r.split(','))))\n",
    "        return np.array(samples)\n",
    "    else:\n",
    "        raise ValueError(\"Thie model only supports text/csv input\")\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    return model.predict(pd.DataFrame(input_data, columns=model.steps[0][1]._feature_names_in))\n",
    "\n",
    "def output_fn(prediction, content_type):\n",
    "    return str(prediction)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--train', type=str, default=\"/opt/ml/input/data/train\")\n",
    "    parser.add_argument('--num_features', type=str) \n",
    "    parser.add_argument('--cat_features', type=str)\n",
    "    parser.add_argument('--model-dir', type=str, default=\"/opt/ml/model\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    train_path = args.train\n",
    "    num_features = args.num_features.split()\n",
    "    cat_features = args.cat_features.split()\n",
    "    model_dir = args.model_dir\n",
    "\n",
    "    X_train = pd.read_csv(os.path.join(train_path, \"X_train.csv\"))\n",
    "    y_train = pd.read_csv(os.path.join(train_path, \"y_train.csv\"))\n",
    "    \n",
    "    preprocessor = make_column_transformer(\n",
    "        (StandardScaler(), num_features),\n",
    "        (OneHotEncoder(sparse=False), cat_features),\n",
    "    )\n",
    "    \n",
    "    model = LogisticRegression(class_weight=\"balanced\", solver=\"lbfgs\")\n",
    "    \n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('model', model)])\n",
    "    \n",
    "    pipeline.fit(X_train, np.ravel(y_train))\n",
    "    \n",
    "    model_output_directory = os.path.join(model_dir, \"model.joblib\")\n",
    "    print(\"Model saving path {}\".format(model_output_directory))\n",
    "    joblib.dump(pipeline, model_output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32556322",
   "metadata": {},
   "source": [
    "The script also contains several serving functions that Sagemaker requires for model serving via the Sagemaker model endpoint service. These functions comprise of model_fn() ensuring that the model gets loaded from file, input_fn() handling the input in a way that it can be used for calling the predict() function on the model, the predict_fn() which calls predict on the model and the output_fn(), which will convert the model output to a format that can be send back to the caller. \n",
    "\n",
    "Unless we are ready to train and deploy on a dedicated container, the script is callable our local machine, resp. on the instance on which the notebook is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0cb044",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 train_and_deploy.py --train ./  \\\n",
    "                             --num_features \"bill_length_mm bill_depth_mm flipper_length_mm\"  \\\n",
    "                             --cat_features \"species island\"  \\\n",
    "                             --model-dir ./  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f6fb80",
   "metadata": {},
   "source": [
    "The SKLearn object is the standard interface for scheduling and defining model training and deployment of scikit-learn models. We specify the resources needed, the framework version, the entry point, the role as well as the output_path which will be the model-dir argument. Further arguments like the numerical and categorical feature list can be passed via the hyperparameters dictionary. \n",
    "Then, we can call fit() to execute the training job. \n",
    "  \n",
    "We pass a dictionary with a single keyword \"train\" that specifies the path to the processed data in S3. The training data is then copied from there to the directory of the training container. The SKLearn object will move the model artifacts to the desired output path in S3, defined via the keyword \"output_path\" in its definition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac73c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.estimator import SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dfd911",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913553b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn = SKLearn(\n",
    "    entry_point=\"train_and_deploy.py\",\n",
    "    framework_version=\"0.23-1\", \n",
    "    instance_type=\"ml.m5.xlarge\", \n",
    "    role=sagemaker_role,\n",
    "    hyperparameters={\n",
    "        \"num_features\": \"bill_length_mm bill_depth_mm flipper_length_mm\",\n",
    "        \"cat_features\": \"species island\"\n",
    "    },\n",
    "    output_path=s3_output_storage_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc532c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.fit({\"train\": s3_data_storage_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a56138",
   "metadata": {},
   "source": [
    "# Model deployment\n",
    "\n",
    "After evaluating our model, we can now go on and deploy it. To do so, we only must call deploy() on the SKlearn object that we used for model training. A model endpoint is now booted in the background. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb82bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = sklearn.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26e4001",
   "metadata": {},
   "source": [
    "We can now run our first test against our model endpoint directly from our jupyter notebook. To do so, we can simply take some of the training features, add them to a request and then call our model by using the Sagemaker client with the invoc_endpoint() method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b9c90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0c04e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_predicted = X_test.head(10).values.tolist()\n",
    "\n",
    "request_body = \"\"\n",
    "for sample in to_be_predicted:\n",
    "    request_body += \",\".join([str(n) for n in sample]) + \"|\"\n",
    "request_body = request_body[:-1] \n",
    "print(\"*\"*20)\n",
    "print(f\"Calling Sagemaker Endopint with the following request_body: {request_body}\")\n",
    "\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "endpoint_name = predictor.endpoint_name\n",
    "content_type = 'text/csv'\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=request_body,\n",
    "    ContentType=content_type\n",
    "    )\n",
    "response_from_endpoint = response['Body'].read().decode(\"utf-8\")\n",
    "print(\"*\"*20)\n",
    "print(f\"Response from Endpoint: {response_from_endpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a42a78",
   "metadata": {},
   "source": [
    "At the end of our journey, the endpoint should be shut down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65efb737",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929ac6a2",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "\n",
    "see WORD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
