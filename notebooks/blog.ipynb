{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3197c45-3d53-4138-9e92-00ba998a0cc2",
   "metadata": {},
   "source": [
    "# Improve your machine learning training routine with cloud-based, containerized workflows \n",
    "**Carsten Frommhold, Paul Elvers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cdb473",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "Training machine learning models on a local machine in a notebook is a common task among data scientists.  It is the easiest way to get started, experiment and build a first working model. But for most businesses this is not a satisfying option: Today, making the most of your training data usually means to scale to gigabytes or petabytes of data, which do not easily fit into your local machine. Data is your most valuable asset and you would not want to use only small fraction of it due to technical reasons. Another problem that might occur when training a model on your local machine is putting it to use. A model that can only be used on your laptop is pretty useless. It should be available to a large group of consumers, deployed as a REST API or making batch predictions in a large data pipeline. If your model “works on your machine”, how does it get to production?\n",
    "\n",
    "Another pitfall lies in the consistency between code used to train a model and the serialized model itself. By design, a notebook is well suited for exploratory analysis and development via trial and error. For example, it is possible to jump between individual cells. It is not enforced to keep a given top-down order. This makes it difficult to check whether the code exactly matches the model fit. Fitting a model in a notebook, persisting it, change the notebook afterwards and do a git commit is not a desired workflow and might lead to non-traceable issues. In an optimal workflow, models and associated code should be kept consistent with each other in the sense that it should be possible to fit the model in a new environment again.\n",
    "\n",
    "We demonstrate how many of the issues can be circumvented, by moving your training routine to the cloud. We use the AWS Service Sagemaker to execute a containerised training routine that will allow us to a) scale our training job to whatever sizes of training data, b) support our routines with single or multiple GPUs if needed, and c) easily deploy our model after the training has succeeded. The training and deployment routine will be written in a python script, which can easily be version controlled, and allows us to link code and model. The script is passed via the Sagemaker Python SDK to a container, running a docker image with all dependencies needed for the job. The main benefit of using Sagemaker is that instead of having to manage the infrastructure running the container, all the infrastructure management is done for you by AWS. All that is required is to make a few calls with the Python Sagemaker SDK and everything else is taken care of."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed534af",
   "metadata": {},
   "source": [
    "## Prerequisites \n",
    "To try this demo out by yourself, you will need access to an AWS account. The easiest way would be to use an AWS Sagemaker notebook instance (you can find more information about how to configure a notebook instance [here](https://docs.aws.amazon.com/sagemaker/latest/dg/howitworks-create-ws.html)), but the code can be run in any compute environment, given that authentication and permissions are provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccac7297",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data\n",
    "\n",
    "In this example we will be using the Palmer Penguin Dataset, which provides a suitable alternative to the frequently used Iris dataset. It contains information about various penguins. You can read more about it [here](https://allisonhorst.github.io/palmerpenguins/articles/intro.html). The objective we will be solving with our machine learning algorithm is predicting the sex of a penguin (male/female) by using various attributes of the penguin (e.g. flipper length, bill length, species, island) as our features.\n",
    "![Penguins](https://camo.githubusercontent.com/4ff5cb2b783a76207da2b80b5b5e97b045f39f05cf76e8a21a34a18dc5f492cd/68747470733a2f2f616c6c69736f6e686f7273742e6769746875622e696f2f70616c6d657270656e6775696e732f6d616e2f666967757265732f6c7465725f70656e6775696e732e706e67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-chuck",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install palmerpenguins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4f9a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from palmerpenguins import load_penguins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c287bc20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>male</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>female</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>female</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
       "0  Adelie  Torgersen            39.1           18.7              181.0   \n",
       "1  Adelie  Torgersen            39.5           17.4              186.0   \n",
       "2  Adelie  Torgersen            40.3           18.0              195.0   \n",
       "\n",
       "   body_mass_g     sex  year  \n",
       "0       3750.0    male  2007  \n",
       "1       3800.0  female  2007  \n",
       "2       3250.0  female  2007  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": null,
   "id": "c287bc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = load_penguins()\n",
    "penguins.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dad42f1",
   "metadata": {},
   "source": [
    "Minimal data procesing is required: We drop entries with null values as well as duplicates. Then, we perform a train-test-split and save the data on our working directory as well as in AWS s3 storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faadbd96-4765-4cac-a2ee-8e79d4662388",
   "metadata": {},
   "source": [
    "We use the library dotenv to load the following environment variables. They will be needed for executing the job further below and contain information on the role used for execution as well as the paths to the data and the model storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b472567-4db8-4a25-b6b4-e9b5599e29eb",
   "id": "734d87cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea98ba5-b5de-421e-9e37-d8c3ddd35fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "sagemaker_role= os.getenv(\"SAGEMAKER_ROLE\")\n",
    "s3_data_storage_path=os.getenv(\"DATA_PATH\")\n",
    "s3_output_storage_path=os.getenv(\"OUTPUT_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec18feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins.dropna(inplace=True)\n",
    "penguins.drop_duplicates(inplace=True)\n",
    "\n",
    "features = [\n",
    "    \"bill_length_mm\",\n",
    "    \"bill_depth_mm\",\n",
    "    \"flipper_length_mm\",\n",
    "    \"species\",\n",
    "    \"island\",\n",
    "]\n",
    "\n",
    "target = \"sex\"\n",
    "\n",
    "test_amount = 0.3\n",
    "train = [np.random.uniform() >= test_amount for _ in range(len(penguins))]\n",
    "test = [not train_flag for train_flag in train]\n",
    "\n",
    "X_train = penguins[train][features]\n",
    "y_train = penguins[train][target]\n",
    "X_test = penguins[test][features]\n",
    "y_test = penguins[test][target]\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"data/\")\n",
    "    print(\"Created data/ directory.\")\n",
    "except FileExistsError:\n",
    "    print(\"Data directory already exists\")\n",
    "    \n",
    "\n",
    "for raw_data_bucket in [\"data/\", s3_data_storage_path]:\n",
    "\n",
    "    X_train.to_csv(os.path.join(raw_data_bucket, \"X_train.csv\"), index=False)\n",
    "    y_train.to_csv(os.path.join(raw_data_bucket, \"y_train.csv\"), index=False)\n",
    "    X_test.to_csv(os.path.join(raw_data_bucket, \"X_test.csv\"), index=False)\n",
    "    y_test.to_csv(os.path.join(raw_data_bucket, \"y_test.csv\"), index=False)\n",
    "    print(f\"Stored data in '{raw_data_bucket}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279c41cc",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "\n",
    "To execute the training and deployment routine, we need to write a python script. The crucial part for the training lies in the *__main__* clause. It reads the data, instantiates a pipeline and trains the the model. Here, a minimal preprocessing of one-hot-encoding and standard scaling is chosen. LogisticRegression acts as a baseline model. The model is then serialized and saved given the model directory. \n",
    "\n",
    "The script takes four arguments. First, we need to define input path for the training data. It assumes the existence of two files: X_train.csv and y_train.csv. The differentiation between categorical and numerical variables is explicitly given in these example. Finally, the output path for the serialized model is defined. Using these arguments makes it convenient to run the training routine on our local machine and via SageMaker in the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c372a236",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_and_deploy.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "import argparse\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    model = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return model\n",
    "\n",
    "def float_if_number(entry):\n",
    "    try:\n",
    "        return float(entry)\n",
    "    except ValueError:\n",
    "        return entry\n",
    "\n",
    "def input_fn(request_body, content_type):\n",
    "    if content_type == 'text/csv':\n",
    "        features = []\n",
    "        for r in request_body.split('|'):\n",
    "            mapped_row = list(map(float_if_number,r.split(',')))\n",
    "            features.append(mapped_row)\n",
    "            features_as_array = np.array(features)\n",
    "        return features_as_array\n",
    "    else:\n",
    "        raise ValueError(\"Thie model only supports text/csv input\")\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    return model.predict(pd.DataFrame(input_data, columns=model.steps[0][1]._feature_names_in))\n",
    "\n",
    "def output_fn(prediction, content_type):\n",
    "    return str(prediction)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--train', type=str, default=\"/opt/ml/input/data/train\")\n",
    "    parser.add_argument('--num_features', type=str) \n",
    "    parser.add_argument('--cat_features', type=str)\n",
    "    parser.add_argument('--model-dir', type=str, default=\"/opt/ml/model\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    train_path = args.train\n",
    "    num_features = args.num_features.split()\n",
    "    cat_features = args.cat_features.split()\n",
    "    model_dir = args.model_dir\n",
    "\n",
    "    X_train = pd.read_csv(os.path.join(train_path, \"X_train.csv\"))\n",
    "    y_train = pd.read_csv(os.path.join(train_path, \"y_train.csv\"))\n",
    "    \n",
    "    preprocessor = make_column_transformer(\n",
    "        (StandardScaler(), num_features),\n",
    "        (OneHotEncoder(sparse=False), cat_features),\n",
    "    )\n",
    "    \n",
    "    model = LogisticRegression(class_weight=\"balanced\", solver=\"lbfgs\")\n",
    "    \n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('model', model)])\n",
    "    \n",
    "    pipeline.fit(X_train, np.ravel(y_train))\n",
    "    \n",
    "    model_output_directory = os.path.join(model_dir, \"model.joblib\")\n",
    "    print(\"Model saving path {}\".format(model_output_directory))\n",
    "    joblib.dump(pipeline, model_output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32556322",
   "metadata": {},
   "source": [
    "The script also contains several serving functions that Sagemaker requires for model serving via the Sagemaker model endpoint service. These functions comprise of model_fn() ensuring that the model gets loaded from file, input_fn() handling the input in a way that it can be used for calling the predict() function on the model, the predict_fn() which calls predict on the model and the output_fn(), which will convert the model output to a format that can be send back to the caller. \n",
    "\n",
    "For testing purposes the script is also callable on our local machine, or on the instance on which the notebook is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0cb044",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 train_and_deploy.py --train ./data  \\\n",
    "                             --num_features \"bill_length_mm bill_depth_mm flipper_length_mm\"  \\\n",
    "                             --cat_features \"species island\"  \\\n",
    "                             --model-dir ./  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f6fb80",
   "metadata": {},
   "source": [
    "In order to run the training routine in the cloud, we use the SKLearn object from the Pyhton SDK. It is the standard interface for scheduling and defining model training and deployment of scikit-learn models. We specify the resources needed, the framework version, the entry point, the role as well as the output_path which will be the model-dir argument. Further arguments like the numerical and categorical feature list can be passed via the hyperparameters dictionary. \n",
    "\n",
    "When calling fit(), Sagemaker will automatically launch a container with a scikit-learn image and execute the training script. The dictonary that we pass with a single keyword \"train\" to the fit() function specifies the path to the processed data in S3. The training data is copied from there into the training container. The SKLearn object will move the model artifacts to the desired output path in S3, defined via the keyword \"output_path\" in its definition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac73c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.estimator import SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913553b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn = SKLearn(\n",
    "    entry_point=\"train_and_deploy.py\",\n",
    "    framework_version=\"0.23-1\", \n",
    "    instance_type=\"ml.m5.xlarge\", \n",
    "    role=sagemaker_role,\n",
    "    hyperparameters={\n",
    "        \"num_features\": \"bill_length_mm bill_depth_mm flipper_length_mm\",\n",
    "        \"cat_features\": \"species island\"\n",
    "    },\n",
    "    output_path=s3_output_storage_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc532c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.fit({\"train\": s3_data_storage_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e124b8-048e-4f8a-894a-89cdcc8541f3",
   "metadata": {},
   "source": [
    "Instantiating the SKLearn object and calling the fit() method are everything that is needed to launch the training routine as a containerized workflow. We could easily scale the resources, for example by choosing a bigger instance or use GPU support for our training. We could also make our calls automatically using a workflow orchestration tool such as Airflow or AWS Step Function, or trigger our training each time we merge our training routine with a CI/CD pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a56138",
   "metadata": {},
   "source": [
    "## Model deployment\n",
    "\n",
    "After evaluating our model, we can now go on and deploy it. To do so, we only have to call deploy() on the SKlearn object that we used for model training. A model endpoint is now booted in the background. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb82bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = sklearn.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d73c9d-d821-404c-9fc2-097ea4499a71",
   "metadata": {},
   "source": [
    "We can test the endpoint by passing the top 10 rows of our test dataset to the endpoint. We will receive labeled predictions as to whether the penguin is a male or a female based on it's attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0c04e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "to_be_predicted = X_test.head(10).values.tolist()\n",
    "\n",
    "request_body = \"\"\n",
    "for sample in to_be_predicted:\n",
    "    request_body += \",\".join([str(n) for n in sample]) + \"|\"\n",
    "request_body = request_body[:-1] \n",
    "print(\"*\"*20)\n",
    "print(f\"Calling Sagemaker Endopint with the following request_body: {request_body}\")\n",
    "\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "endpoint_name = predictor.endpoint_name\n",
    "content_type = 'text/csv'\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=request_body,\n",
    "    ContentType=content_type\n",
    "    )\n",
    "response_from_endpoint = response['Body'].read().decode(\"utf-8\")\n",
    "print(\"*\"*20)\n",
    "print(f\"Response from Endpoint: {response_from_endpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a42a78",
   "metadata": {},
   "source": [
    "At the end of our journey, the endpoint should be shut down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65efb737",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bc23bf-7a5d-4a83-9730-96c426d4f02c",
   "metadata": {},
   "source": [
    "## Outlook\n",
    "We demonstrated how a machine learning training routine can be moved to a cloud-based execution using a dedicated container in AWS. Compared to a local workflow it provides numerous advantages, such as scalability of compute ressources (one simply has to change parameters on the SKLearn object) and reproducibility of results: training scripts can easily be versioned and training jobs automatically be triggered, allowing to connect model training and version control. Most importantly, it reduces the barrier between development and production: Deploying a trained model only requires a single method call.  \n",
    "\n",
    "Of course, the example provided here is not solving all technical hurdles of MLOps and more features are usually needed to build a mature ml-platform. Ideally, we would want to have a model registry, where we store and manage models and artifacts, an experimentation tracking platform to log all our efforts to improve the model, and perhaps also a data versioning tool. But moving your local training routine into the cloud is already a big step forward!\n",
    "\n",
    "There are also many possibilities to extend the training routine and adapt it towards specific needs: If special python libraries or dependencies are used for the algorithm, a custom docker image can be pushed to AWS Elastic Container Registry (ECR) and then be used in the Sagemaker training routine. If data processing becomes more complex, one can use a processing container to decapsulate this step. Also, if the endpoint is used in production, it is advisable to develop a REST API on top of the deployed Sagemaker endpoint, allowing to better handle security constraints as well as logical heuristics and preprocessing of your API calls. And of course a model evaluation step that calculates metrics on the vaildation data set should be included, but we will dive deeper into model evalutation in one of our next blog posts. Stay tuned!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
