{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-florist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from palmerpenguins import load_penguins\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-vegetable",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-buffer",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = load_penguins()\n",
    "penguins.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-carter",
   "metadata": {},
   "source": [
    "## Drop nas, dups and train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-weapon",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins.dropna(inplace=True)\n",
    "penguins.drop_duplicates(inplace=True)\n",
    "\n",
    "features = [\n",
    "    \"bill_length_mm\",\n",
    "    \"bill_depth_mm\",\n",
    "    \"flipper_length_mm\",\n",
    "    \"species\",\n",
    "    \"island\",\n",
    "]\n",
    "\n",
    "target = \"sex\"\n",
    "\n",
    "test_amount = 0.3\n",
    "train = [np.random.uniform() >= test_amount for _ in range(len(penguins))]\n",
    "test = [not train_flag for train_flag in train]\n",
    "\n",
    "X_train = penguins[train][features]\n",
    "y_train = penguins[train][target]\n",
    "X_test = penguins[test][features]\n",
    "y_test = penguins[test][target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-wrestling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speichern sowohl lokal wie auf s3 \n",
    "# --> Nur relevant, wenn wir lokalen Call behalten wollen..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-consumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "for raw_data_bucket in [\"./\"]:\n",
    "\n",
    "    X_train.to_csv(os.path.join(raw_data_bucket, \"X_train.csv\"), index=False)\n",
    "    y_train.to_csv(os.path.join(raw_data_bucket, \"y_train.csv\"), index=False)\n",
    "    X_test.to_csv(os.path.join(raw_data_bucket, \"X_test.csv\"), index=False)\n",
    "    y_test.to_csv(os.path.join(raw_data_bucket, \"y_test.csv\"), index=False)\n",
    "    print(f\"Stored data in '{raw_data_bucket}' .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-adoption",
   "metadata": {},
   "source": [
    "## Train and deploy file\n",
    "\n",
    "\n",
    "**Notizen Carsten**\n",
    "\n",
    "2 Änderungen bei neuer sklearn Version:\n",
    "\n",
    "\n",
    "```console\n",
    "from sklearn.externals import joblib\n",
    "```\n",
    "deprecated since 0.23 --> import joblib directly\n",
    "\n",
    "\n",
    "```console\n",
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), num_features),\n",
    "    (OneHotEncoder(sparse=False), cat_features),\n",
    ")\n",
    "```\n",
    "different position of variables ([see more](https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_transformer.html))\n",
    "\n",
    "\n",
    "**Pfade auf Docker**  \n",
    "Default für Training-Input \"/opt/ml/input/data/train\" und Model-Dir sind \"/opt/ml/model\"  \n",
    "\n",
    "Alternativ sind diese via os.environ.get(\"SM_MODEL_DIR\") bzw os.environ.get(\"SM_CHANNEL_TRAIN\") auf dem prebuilt sklearn docker schon gesetzt. \n",
    "\n",
    "Hierher werden bei Call des Wrappers potentiell Inputs/Outputs von s3 kopiert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-ending",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_and_deploy.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "import argparse\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    model = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return model\n",
    "\n",
    "def input_fn(request_body, content_type):\n",
    "    if content_type == 'text/csv':\n",
    "        samples = []\n",
    "        for r in request_body.split('|'):\n",
    "            samples.append(list(map(float,r.split(','))))\n",
    "        return np.array(samples)\n",
    "    else:\n",
    "        raise ValueError(\"Thie model only supports text/csv input\")\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    return model.predict(input_data)\n",
    "\n",
    "def output_fn(prediction, content_type):\n",
    "    return str(prediction)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--train', type=str, default=\"/opt/ml/input/data/train\")\n",
    "    parser.add_argument('--num_features', type=str) \n",
    "    parser.add_argument('--cat_features', type=str)\n",
    "    parser.add_argument('--model-dir', type=str, default=\"/opt/ml/model\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    train_path = args.train\n",
    "    num_features = args.num_features.split()\n",
    "    cat_features = args.cat_features.split()\n",
    "    model_dir = args.model_dir\n",
    "\n",
    "    X_train = pd.read_csv(os.path.join(train_path, \"X_train.csv\"))\n",
    "    y_train = pd.read_csv(os.path.join(train_path, \"y_train.csv\"))\n",
    "    \n",
    "    preprocessor = make_column_transformer(\n",
    "        (StandardScaler(), num_features),\n",
    "        (OneHotEncoder(sparse=False), cat_features),\n",
    "    )\n",
    "    \n",
    "    model = LogisticRegression(class_weight=\"balanced\", solver=\"lbfgs\")\n",
    "    \n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('model', model)])\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    model_output_directory = os.path.join(model_dir, \"model.joblib\")\n",
    "    print(\"Model saing path {}\".format(model_output_directory))\n",
    "    joblib.dump(pipeline, model_output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-orleans",
   "metadata": {},
   "source": [
    "### Local call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-instrument",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 train_and_deploy.py --train ./  \\\n",
    "                             --num_features \"bill_length_mm bill_depth_mm flipper_length_mm\"  \\\n",
    "                             --cat_features \"species island\"  \\\n",
    "                             --model-dir ./  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-junior",
   "metadata": {},
   "source": [
    "### SKlearn API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-sacrifice",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn = SKLearn(\n",
    "    entry_point=\"train_and_deploy.py\",\n",
    "    framework_version=\"1.0.1\", \n",
    "    instance_type=\"ml.m5.xlarge\", \n",
    "    role=sagemaker_role,\n",
    "    hyperparameters={\n",
    "        \"num_features\": \"bill_length_mm bill_depth_mm flipper_length_mm\",\n",
    "        \"cat_features\": \"species island\"\n",
    "    }\n",
    ")\n",
    "sklearn.fit({\"train\": processed_data_bucket})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-shoot",
   "metadata": {},
   "source": [
    "Beschreiben, was hier passiert! \\\n",
    "Wenn wir train mitgeben, wird automatisch ProcessingInput des Ordners data_bucket mit Destination \"/opt/ml/input/data/train\" im Docker betrieben.\n",
    "\n",
    "Serialisiertes Modell wird bei nicht-spezifizieren entsprechend in s3 abgelegt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-beast",
   "metadata": {},
   "source": [
    "### Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-dayton",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-fountain",
   "metadata": {},
   "source": [
    "Hier könnte man Evaluierung des Modells wieder im Notebook machen. \\\n",
    "y_pred via Endpunkt Invocation and metrics(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-means",
   "metadata": {},
   "source": [
    "## Outlook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-cathedral",
   "metadata": {},
   "source": [
    "siehe Word Dokument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-classification",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
